print "hola mundi"
print()
print("hola")
clear
clear
rm(list = ls())
rm(list = ls())
cld
as
as
a
print("asdasdadsasd")
zoo
zoo
print("hola mikky")
print("hehe")
hehe
print('heheheh')
print('asdasdasd')
a = 3
b = 5
c = a + b
print(c)
if a > 1 print('sdad')
if (a > 1) print('sdad')
if (a > 1) print('sdad')
if (a > 1) print('sdad')
asdfhjkl
asdfhjkl''
gustavo
int a = 123
char s = "a"
print("hehe")
print("hola mundo")
print("hehe")
print(5)
print(x)
print(x)
x<-5
x<-5
print(x)
?read.table()
x<-5
print(x)
?read.table()
?mtcars
mtcars
datos<-mtcars
str(datos)
summary(datos)
datos[,]
datos[,2]
datos[,1]
datos[,"hp"]
datos[,"hp"]
datos[1,"hp"]
datos[3,"hp"]
datos[,"hp"]
datos$qsec
datos$hp
datos[,"hp"]
datos$hp
datos[,c("wt", "hp")]
summary(datos)
#setwd("C:/Users/alber/Documents/UVG/Septimo semestre/Mineria de Datos/Hoja_Trabajo_3/HDT3-Mineria")
setwd("C:/Users/Gustavo/Desktop/SEPTIMO SEMESTRE/MINERIA/HDT3/HDT3-Mineria")
library(rpart)
library(caret)
library(tree)
library(rpart.plot)
library(randomForest)
library(cluster) #Para calcular la silueta
library(e1071)#para cmeans
library(mclust) #mixtures of gaussians
library(fpc) #para hacer el plotcluster
library(NbClust) #Para determinar el número de clusters óptimo
library(factoextra) #Para hacer gráficos bonitos de clustering
library(tidyr)
library(splitstackshape)
test <- read.csv("test.csv", stringsAsFactors = FALSE)
train <- read.csv("train.csv", stringsAsFactors = FALSE)
View(head(train))
summary(train)
trainImportantes <- train[c("MSSubClass","LotFrontage","LotArea","OverallCond","YearBuilt","YearRemodAdd","X2ndFlrSF","FullBath","TotRmsAbvGrd","KitchenAbvGr","GarageCars","PoolArea","SalePrice")]
trainImportantes[is.na(trainImportantes)]<-0
#Para saber cual es el mejor numero de clusters
wss <- (nrow(trainImportantes)-1)*sum(apply(trainImportantes,2,var))
for (i in 2:10)
wss[i] <- sum(kmeans(trainImportantes, centers=i)$withinss)
# Se plotea la grafica de codo
plot(1:10, wss, type="b", xlab="Number of Clusters",  ylab="Within groups sum of squares")
trainImportantes <- train[c("MSSubClass","LotFrontage","LotArea","OverallCond","YearBuilt","YearRemodAdd","X2ndFlrSF","FullBath","TotRmsAbvGrd","KitchenAbvGr","GarageCars","PoolArea","SalePrice")]
trainImportantes[is.na(trainImportantes)]<-0
for (i in 2:10)
wss[i] <- sum(kmeans(trainImportantes, centers=i)$withinss)
# Se plotea la grafica de codo
plot(1:10, wss, type="b", xlab="Number of Clusters",  ylab="Within groups sum of squares")
km<-kmeans(trainImportantes,3)
train$grupo<-km$cluster
head(train)
View(head(train))
plotcluster(trainImportantes,km$cluster)
#Visualización de las k-medias
fviz_cluster(km, data = trainImportantes,geom = "point", ellipse.type = "norm")
#-----------------------------------------------------------------------------------------------
#Silueta de que tan bien hizo el cluster
silkm<-silhouette(km$cluster,dist(trainImportantes))
mean(silkm[,3])
trainImportantes <- train[c("MSSubClass","LotArea","OverallCond","YearBuilt","YearRemodAdd","X2ndFlrSF","FullBath","TotRmsAbvGrd","KitchenAbvGr","GarageCars","PoolArea","SalePrice")]
trainImportantes[is.na(trainImportantes)]<-0
#Para saber cual es el mejor numero de clusters
wss <- (nrow(trainImportantes)-1)*sum(apply(trainImportantes,2,var))
for (i in 2:10)
wss[i] <- sum(kmeans(trainImportantes, centers=i)$withinss)
# Se plotea la grafica de codo
plot(1:10, wss, type="b", xlab="Number of Clusters",  ylab="Within groups sum of squares")
km<-kmeans(trainImportantes,3)
train$grupo<-km$cluster
plotcluster(trainImportantes,km$cluster)
#Visualización de las k-medias
fviz_cluster(km, data = trainImportantes,geom = "point", ellipse.type = "norm")
#-----------------------------------------------------------------------------------------------
#Silueta de que tan bien hizo el cluster
silkm<-silhouette(km$cluster,dist(trainImportantes))
mean(silkm[,3])
g1<- datos[datos$grupo==1,]
head(datos)
#setwd("C:/Users/alber/Documents/UVG/Septimo semestre/Mineria de Datos/Hoja_Trabajo_3/HDT3-Mineria")
setwd("C:/Users/Gustavo/Desktop/SEPTIMO SEMESTRE/MINERIA/HDT3/HDT3-Mineria")
library(rpart)
library(caret)
library(tree)
library(rpart.plot)
library(randomForest)
library(cluster) #Para calcular la silueta
library(e1071)#para cmeans
library(mclust) #mixtures of gaussians
library(fpc) #para hacer el plotcluster
library(NbClust) #Para determinar el número de clusters óptimo
library(factoextra) #Para hacer gráficos bonitos de clustering
library(tidyr)
library(splitstackshape)
test <- read.csv("test.csv", stringsAsFactors = FALSE)
train <- read.csv("train.csv", stringsAsFactors = FALSE)
trainImportantes <- train[c("MSSubClass","LotFrontage","LotArea","OverallCond","YearBuilt","YearRemodAdd","X2ndFlrSF","FullBath","TotRmsAbvGrd","KitchenAbvGr","GarageCars","PoolArea","SalePrice")]
trainImportantes[is.na(trainImportantes)]<-0
#Para saber cual es el mejor numero de clusters
wss <- (nrow(trainImportantes)-1)*sum(apply(trainImportantes,2,var))
for (i in 2:10)
wss[i] <- sum(kmeans(trainImportantes, centers=i)$withinss)
# Se plotea la grafica de codo
plot(1:10, wss, type="b", xlab="Number of Clusters",  ylab="Within groups sum of squares")
km<-kmeans(trainImportantes,3)
train$grupo<-km$cluster
plotcluster(trainImportantes,km$cluster)
#Visualización de las k-medias
fviz_cluster(km, data = trainImportantes,geom = "point", ellipse.type = "norm")
#-----------------------------------------------------------------------------------------------
#Silueta de que tan bien hizo el cluster
silkm<-silhouette(km$cluster,dist(trainImportantes))
mean(silkm[,3])
g1<- train[train$grupo==1,]
prop11 <- prop.table(table(g1$categoria1))*100
prop12 <- prop.table(table(g1$categoria2))*100
prop13 <- prop.table(table(g1$categoria3))*100
View(head(prop11))
prop11
View(head(g1))
View(head(g2))
g2<- train[train$grupo==2,]
View(head(g2))
g3<- train[train$grupo==3,]
View(head(g2))
View(head(g3))
View(head(g3,15))
View(head(g1,15))
View(head(g2,15))
View)g1)
View(g1)
